{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Library\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Penetapan Seed untuk Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbed32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Konfigurasi Path dan Parameter\n",
    "TRAIN_DIR = 'data/processed/processed_train'\n",
    "VAL_DIR = 'data/processed/processed_val'\n",
    "TEST_DIR = 'data/processed/processed_test'\n",
    "TRAIN_CSV = 'data/train.csv'\n",
    "VAL_CSV = 'data/val.csv'\n",
    "TEST_CSV = 'data/test.csv'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20\n",
    "NUM_CLASSES = 4  # Severity 1-4\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "NUM_WORKERS = 0 if platform.system() == \"Windows\" else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23278e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Class\n",
    "class FundusProcessedDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 0]\n",
    "        label = int(self.data.iloc[idx, 1]) - 1  # Label severity 1-4 â†’ 0-3\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Transformasi dan DataLoader\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = FundusProcessedDataset(TRAIN_CSV, TRAIN_DIR, transform=train_transform)\n",
    "val_dataset = FundusProcessedDataset(VAL_CSV, VAL_DIR, transform=val_test_transform)\n",
    "test_dataset = FundusProcessedDataset(TEST_CSV, TEST_DIR, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model, Loss, Optimizer, Scheduler\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Loop dengan Early Stopping & Visualisasi Progress\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Training\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # --- VALIDASI ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Validation\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # --- EARLY STOPPING & SIMPAN MODEL TERBAIK ---\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'efficientnetb0_best.pth')\n",
    "        print(\"Model terbaik disimpan.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualisasi Loss & Akurasi per Epoch\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluasi pada Data Validasi\n",
    "print(\"\\nEvaluasi pada data validasi:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)]))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Validation)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76245ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluasi pada Data Test\n",
    "model.load_state_dict(torch.load('efficientnetb0_best.pth'))\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"\\nEvaluasi pada data test:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)]))\n",
    "cm_test = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)],\n",
    "            yticklabels=[f\"Severity {i+1}\" for i in range(NUM_CLASSES)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
